import csv
from datetime import datetime, timedelta
import os
import random
import time

from feast import (
    Entity,
    FeatureView,
    Field,
    FileSource,
    FeatureStore,
)
from feast.types import Int64


import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa

import google.auth
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError


def nanoseconds_to_milliseconds(time_ns):
    """Converts a time value in nanoseconds to milliseconds.

    :param time_ns: A numeric time value in nanoseconds. An example would be
        a value returned by `time.perf_counter_ns`.

    :return time_ms: `time_ns` converted to milliseconds.
    """
    return time_ns / 1e6


def generate_timestamps(num, start=datetime(2023, 1, 1), end=datetime(2024, 1, 1)):
    """Generates a list of random timestamps in a given range of time.

    :param num: The number of timestamps to generate.
    :param start: The datetime which represents the earliest datetime possible.
    :param end: The datetime which represents the latest datetime possible.

    :return timestamps: A list of timestamps that each fall within the range
        of (start, end).
    """
    return [random.random() * (end - start) + start for _ in range(num)]


def raw_parquet_write_test(filename, num_columns=10, num_rows=10):
    """Writes a parquet file with random-ish data using pyarrow's parquet
    module and times the write operation.

    :param filename: The filename of where to write the parquet file.
    :param num_columns: The number of columns to use when generating the
        data to write to the parquet file.
    :param num_rows: The number of rows to use when generating the
        data to write to the parquet file.

    :return (time_ms, filesize, timestamps):
        time_ms - The time it took to write the parquet file in milliseconds.
        filesize - The size of the resulting file in bytes.
        timestamps - The list of timestamps used in the parquet file.
    """
    data = {}
    data["id"] = list(range(1, num_rows + 1))
    timestamps = generate_timestamps(num_rows)
    data["event_timestamp"] = timestamps
    for i in range(1, num_columns + 1):
        data[f"{i}"] = list(range(num_rows))

    df = pd.DataFrame(data)
    table = pa.Table.from_pandas(df)

    begin = time.perf_counter_ns()
    pq.write_table(table, filename)
    end = time.perf_counter_ns()

    filesize = os.path.getsize(filename)

    return nanoseconds_to_milliseconds(end - begin), filesize, timestamps


def raw_parquet_read_test(filename):
    """Reads a parquet file using pyarrow's parquet module.

    :param filename: The filename to read.

    :return time_ms: The time it took to read the file in milliseconds.
    """
    begin = time.perf_counter_ns()
    _ = pq.read_table(filename)
    end = time.perf_counter_ns()

    return nanoseconds_to_milliseconds(end - begin)


def generate_feast_repository_definitions(
    num_columns, parquet_filename="./outfile.parquet"
):
    """Generates feast repository definitions needed to access the parquet
    file generated by `raw_parquet_write_test`.

    :param num_columns: The number of columns in the parquet file.
    :param parquet_filename: The name of the parquet file.

    :return definitions: The feast `Entity`, `FileSource`, and
        FeatureView definitions necessary for `FeatureStore.apply` as a list.
    """
    dummy_entity = Entity(name="dummy", join_keys=["id"])
    dummy_source = FileSource(
        name="dummy_source",
        # path="/home/clif/git_repos/feast_eval/parquet/feature_repo/outfile.parquet",
        path=parquet_filename,
        timestamp_field="event_timestamp",
    )

    dummy_fv = FeatureView(
        # The unique name of this feature view. Two feature views in a single
        # project cannot have the same name
        name="dummy_stats",
        entities=[dummy_entity],
        ttl=timedelta(days=1),
        # The list of features defined below act as a schema to both define features
        # for both materialization of features into a store, and are used as references
        # during retrieval for building a training dataset or serving features
        schema=[Field(name=f"{i}", dtype=Int64) for i in range(1, num_columns + 1)],
        online=True,
        source=dummy_source,
        # Tags are user defined key/value pairs that are attached to each
        # feature view
        tags={"dummy": "dummy_stats"},
    )
    return [dummy_entity, dummy_source, dummy_fv]


def offline_read_test(num_columns, num_rows, timestamps):
    """Conducts a test to time reading a parquet file via Feast's
    `get_historical_features` API.

    :param: The number of columns in the parquet file.
    :param: The number of rows in the parquet file.
    :param: The list of timestamps used to generate the parquet file initially.

    :return time_ms, feature_df:
        time_ms - Time it took to run `get_historical_features` and `to_df`.
        feature_df - The parquet file data in a pandas dataframe.
    """
    store = FeatureStore(repo_path=".")

    # Why do we have to apply every time? Or do we?
    defs = generate_feast_repository_definitions(num_columns)
    store.apply(defs)

    entity_df = pd.DataFrame.from_dict(
        {
            # entity's join key -> entity values
            "id": list(range(num_rows)),
            # "event_timestamp" (reserved key) -> timestamps
            # TODO: Can I specify a range?
            "event_timestamp": timestamps,
        }
    )

    begin = time.perf_counter_ns()
    feature_df = store.get_historical_features(
        entity_df=entity_df,
        features=[f"dummy_stats:{i}" for i in range(1, num_columns + 1)],
    ).to_df()
    end = time.perf_counter_ns()

    return nanoseconds_to_milliseconds(end - begin), feature_df


def offline_write_test(data, num_columns):
    """Conducts a test to time writing a parquest file via Feast's
    `write_to_offline_store` API.

    :param data: The data as a pandas dataframe to write to the parquet file.
    :param num_columns: The number of columns in the dataframe.

    :return time_ms: The time in milliseconds it takes to call `write_to_offline_store`.
    """
    store = FeatureStore(repo_path=".")

    # Why do we have to apply every time? Or do we?
    defs = generate_feast_repository_definitions(num_columns)
    store.apply(defs)

    begin = time.perf_counter_ns()
    store.write_to_offline_store("dummy_stats", data)
    end = time.perf_counter_ns()

    return nanoseconds_to_milliseconds(end - begin)


def write_results_to_csv_file(results, outfilename):
    """Writes the results of tests to a csv file.

    :param results: The results of testing as a list of lists.
    :param outfilename: The name of the file to write results.
    """
    with open(outfilename, "w", newline="\n") as csvfile:
        testwriter = csv.writer(csvfile, delimiter=",")
        for row in results:
            testwriter.writerow(row)


# TODO: In progress, DOES NOT WORK.
def write_results_to_google_sheet(results):
    creds, _ = google.auth.default()

    try:
        service = build("sheets", "v4", credentials=creds)
        spreadsheet = {"properties": {"title": "test-title"}}
        spreadsheet = (
            service.spreadsheets()
            .create(body=spreadsheet, fields="spreadsheetId")
            .execute()
        )

    except HttpError as error:
        print(f"Error creating spreadsheet: {error}")


def run_tests(
    parquet_outfile="outfile.parquet", csv_outfile="parquet_test_results.csv"
):
    """Runs a series of tests on feast's parquet file read/write performance.

    :param parquet_outfile: The name of the file to use for generated parquet
        test data.
    :param csv_outfile: The name of the csv file to use to record results.
    """

    # Tuples of (num_columns, num_rows) to use for each group of test runs.
    tests = [
        (10, 10),
        (10, 100),
        (10, 1000),
        (10, 10000),
        # (100, 10),
        # (100, 100),
        # (100, 1000),
        # (100, 10000),
        # (1000, 10),
        # (1000, 100),
        # (1000, 1000),
        # (1000, 10000),
        # (10000, 10),
        # (10000, 100),
        # (10000, 1000),
        # (10000, 10000),
    ]

    print("Running tests....")
    results = []
    results.append(
        [
            "Number of columns",
            "Number of rows",
            "Raw write in ms",
            "file size in bytes",
            "Raw read in ms",
            "get_historical_feature read in ms",
            "write_to_offline_store write in ms",
        ]
    )
    print(f"Using {parquet_outfile} as the parquet file.")
    for test in tests:
        num_columns = test[0]
        num_rows = test[1]
        print(f"Running test (cols: {num_columns}, rows: {num_rows})")
        elapsed_time_in_ms_write, filesize, timestamps = raw_parquet_write_test(
            parquet_outfile, num_columns, num_rows
        )
        elapsed_time_in_ms_read = raw_parquet_read_test(parquet_outfile)

        elapsed_time_in_ms_offline_read, data = offline_read_test(
            num_columns, num_rows, timestamps
        )
        elapsed_time_in_ms_offline_write = offline_write_test(data, num_columns)
        results.append(
            [
                num_columns,
                num_rows,
                elapsed_time_in_ms_write,
                filesize,
                elapsed_time_in_ms_read,
                elapsed_time_in_ms_offline_read,
                elapsed_time_in_ms_offline_write,
            ]
        )

    print(f"Using {csv_outfile} as the csv results file.")
    write_results_to_csv_file(results, csv_outfile)
    print("done!")


if __name__ == "__main__":
    run_tests()
